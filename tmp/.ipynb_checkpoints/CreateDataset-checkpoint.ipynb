{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T12:31:14.853851Z",
     "start_time": "2019-06-18T12:31:13.540461Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import torch\n",
    "import torchvision \n",
    "import h5py\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from dataset import get_mean_std\n",
    "\n",
    "from keras.datasets import mnist, cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:15:54.998791Z",
     "start_time": "2019-06-17T07:15:54.734049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:16:06.438331Z",
     "start_time": "2019-06-17T07:16:05.749588Z"
    }
   },
   "outputs": [],
   "source": [
    "img_channels, img_rows, img_cols = 1, 28, 28 # mnist\n",
    "# img_channels, img_rows, img_cols = 3, 32, 32 #cifar\n",
    "X_train = X_train.reshape(X_train.shape[0], img_channels, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_channels, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "y_test = y_test.reshape(y_test.shape[0])\n",
    "X_train = np.vstack((X_train,X_test))\n",
    "y_true = np.hstack((y_train,y_test))\n",
    "y_true = y_true.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:16:07.618212Z",
     "start_time": "2019-06-17T07:16:07.600257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 1, 28, 28)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_true.shape)\n",
    "merge_data = X_train\n",
    "merge_targets = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:15:11.312386Z",
     "start_time": "2019-06-17T07:15:11.288541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 1, 28)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "merge_data = merge_data.reshape(merge_data.shape[0], -1, merge_data.shape[1], merge_data.shape[2])\n",
    "print(merge_data.shape)\n",
    "print(merge_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:16:12.846985Z",
     "start_time": "2019-06-17T07:16:11.652907Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.13092558], dtype=float32), array([0.3084485], dtype=float32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_std(merge_data, ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Create MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:20:10.273369Z",
     "start_time": "2019-06-17T07:20:10.242044Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "dataset_name = \"mnist.hdf5\"\n",
    "ab_path = os.path.join(data_dir, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:20:14.902138Z",
     "start_time": "2019-06-17T07:20:14.885950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Image for pytorch Dataset should be (samples, height, width, channels)\n",
    "merge_data = merge_data.transpose((0, 2, 3, 1))\n",
    "print(merge_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:21:34.337984Z",
     "start_time": "2019-06-17T07:20:29.950037Z"
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(ab_path, \"w\") as f:\n",
    "    grp = f.create_group(\"mnist\")\n",
    "    grp.create_dataset(\"data\", merge_data.shape, dtype=merge_data.dtype)\n",
    "    grp.create_dataset(\"targets\", merge_targets.shape, dtype=merge_targets.dtype)\n",
    "    grp[\"data\"][:,:,:,:] = merge_data\n",
    "    grp[\"targets\"][:] = merge_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Load Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T07:15:57.928949Z",
     "start_time": "2019-06-17T07:15:57.741459Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n",
      "(10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() #cifar10\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_channels, img_rows, img_cols = 3, 32, 32 #cifar\n",
    "X_train = X_train.reshape(X_train.shape[0], img_channels, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_channels, img_rows, img_cols)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "y_test = y_test.reshape(y_test.shape[0])\n",
    "X_train = np.vstack((X_train,X_test))\n",
    "y_true = np.hstack((y_train,y_test))\n",
    "y_true = y_true.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3, 32, 32)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_true.shape)\n",
    "merge_data = X_train\n",
    "merge_targets = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 3, 32)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "merge_data = merge_data.reshape(merge_data.shape[0], -1, merge_data.shape[1], merge_data.shape[2])\n",
    "print(merge_data.shape)\n",
    "print(merge_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Create Cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "dataset_name = \"cifar10.hdf5\"\n",
    "ab_path = os.path.join(data_dir, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Image for pytorch Dataset should be (samples, height, width, channels)\n",
    "merge_data = merge_data.transpose((0, 2, 3, 1))\n",
    "print(merge_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(ab_path, \"w\") as f:\n",
    "    grp = f.create_group(\"cifar10\")\n",
    "    grp.create_dataset(\"data\", merge_data.shape, dtype=merge_data.dtype)\n",
    "    grp.create_dataset(\"targets\", merge_targets.shape, dtype=merge_targets.dtype)\n",
    "    grp[\"data\"][:,:,:,:] = merge_data\n",
    "    grp[\"targets\"][:] = merge_targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
